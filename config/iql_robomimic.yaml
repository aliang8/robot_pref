algorithm: iql

data:
  data_path: "/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt"
  reward_model_path: "/scr/matthewh6/robot_pref/reward_model/state_action_reward_model.pt"
  max_segments: 0
  reward_batch_size: 32
  use_ground_truth: False  # Set to True to use ground truth rewards instead of reward model
  scale_rewards: False     # Set to True to scale rewards to min/max range
  reward_min: -1.0         # Minimum value for scaled rewards
  reward_max: 1.0          # Maximum value for scaled rewards
  use_zero_rewards: False  # Sanity check mode: use zero rewards for all transitions
  env_name: "lift"         # Exact name of the RoboMimic environment

model:
  hidden_dims: [256, 256]

iql:
  actor_learning_rate: 1e-4
  critic_learning_rate: 1e-4
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  n_critics: 2
  expectile: 0.7
  weight_temp: 1.0

training:
  n_epochs: 100
  n_steps_per_epoch: 1000
  eval_interval: 10
  eval_episodes: 10
  save_interval: 10

evaluation:
  record_video: True
  parallel_eval: False

output:
  output_dir: "results/iql"
  model_dir_name: "DATASET_NAME_iql_temp${iql.weight_temp}_exp${iql.expectile}_gt${data.use_ground_truth}"

defaults:
  - common
  - _self_  # Allow this config to override common settings
