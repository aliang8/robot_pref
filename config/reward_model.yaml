defaults:
  - common
  - _self_ 

data:
  data_path: "/scr/shared/clam/datasets/metaworld/assembly-v2/buffer_assembly-v2.pt"
  segment_length: 32
  num_segments: 10000
  num_pairs: 50000

  preferences_data_path: null
  normalize_obs: false
  norm_method: "standard"
  
model:
  hidden_dims: [256, 256]
  lr: 3e-4

training:
  batch_size: 256
  num_epochs: 100
  num_workers: 4
  pin_memory: true

hardware:
  gpu: 0
  use_cpu: false

output:
  output_dir: results/reward_model
  model_dir_name: "DATASET_NAME_model_seg${data.segment_length}_epochs${training.num_epochs}_pairs${data.num_pairs}_seed${random_seed}"
  
wandb:
  tags: ["reward_model"]
  notes: "Training reward model with Bradley-Terry preference learning"
