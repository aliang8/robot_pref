# Hydra config for reward model training

defaults:
  - _self_
  - override hydra: default
  - override hydra/launcher: submitit_slurm

# Data parameters
data:
  data_path: "/scr/shared/clam/datasets/metaworld/assembly-v2/buffer_assembly-v2.pt"
  segment_length: 32
  num_segments: 5000
  num_pairs: 500
  preferences_data_path: null
  
# Model architecture
model:
  hidden_dims: [256, 256]
  lr: 1e-4

# Training parameters
training:
  batch_size: 256
  num_epochs: 50
  num_workers: 4
  pin_memory: true

# Hardware settings
hardware:
  gpu: 0
  use_cpu: false

# Output parameters
output:
  output_dir: /scr/aliang80/robot_pref/reward_model
  
# Wandb configuration
wandb:
  use_wandb: true
  project: "robot_preference_learning"
  entity: "clvr"  # Set to your wandb entity or leave as null
  name: "reward_model"  # Will be auto-generated if null
  tags: ["reward_model", "preference_learning"]
  notes: "Training reward model with Bradley-Terry preference learning"

# Hydra configuration settings
hydra:
  job:
    chdir: true
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO
      handlers: [file, console]
    disable_existing_loggers: false
  output_subdir: .hydra 