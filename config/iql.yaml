# Complete IQL configuration file
algorithm: iql

# Include common settings
defaults:
  - common
  - _self_  # Allow this config to override common settings

# Data parameters
data:
  data_path: "/scr/shared/clam/datasets/metaworld/assembly-v2/buffer_assembly-v2.pt"
  reward_model_path: "reward_model/state_action_reward_model.pt"
  max_segments: 1000
  reward_batch_size: 32
  use_ground_truth: false  # Set to true to use ground truth rewards instead of reward model
  env_name: "assembly-v2-goal-observable"  # Exact name of the MetaWorld environment

# Model architecture
model:
  hidden_dims: [256, 256]
  actor_learning_rate: 3e-4
  critic_learning_rate: 3e-4
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  n_critics: 2
  expectile: 0.7
  weight_temp: 3.0
  encoder_dims: [256, 256, 256]

# Training parameters specific to IQL
training:
  n_epochs: 100

# Output parameters
output:
  output_dir: "iql_model"

# Wandb configuration specific to IQL
wandb:
  tags: ["iql", "preference_learning"]
  notes: "IQL policy training with learned reward function" 