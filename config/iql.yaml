# Complete IQL configuration file
algorithm: iql

# Include common settings
defaults:
  - common
  - _self_  # Allow this config to override common settings

# Data parameters
data:
  data_path: "/scr/shared/clam/datasets/metaworld/assembly-v2/buffer_assembly-v2.pt"
  reward_model_path: "reward_model/state_action_reward_model.pt"
  max_segments: 1000
  reward_batch_size: 32
  use_ground_truth: false  # Set to true to use ground truth rewards instead of reward model
  scale_rewards: false     # Set to true to scale rewards to min/max range
  reward_min: -1.0         # Minimum value for scaled rewards
  reward_max: 1.0          # Maximum value for scaled rewards
  use_zero_rewards: false  # Sanity check mode: use zero rewards for all transitions
  env_name: "assembly-v2-goal-observable"  # Exact name of the MetaWorld environment

# Model architecture
model:
  hidden_dims: [256, 256] # this is for the reward model
  actor_learning_rate: 3e-4
  critic_learning_rate: 3e-4
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  n_critics: 2
  expectile: 0.7
  weight_temp: 3.0
  encoder_dims: [256, 256, 256]

# Training parameters specific to IQL
training:
  n_epochs: 100

# Output parameters
output:
  output_dir: "iql_model"
  model_dir_name: "DATASET_NAME_iql_temp${model.weight_temp}_exp${model.expectile}_gt${data.use_ground_truth}"
  artifact_name: "iql_${data.env_name}_DATASET_NAME_temp${model.weight_temp}_exp${model.expectile}"

# Wandb configuration specific to IQL
wandb:
  tags: ["iql", "preference_learning"]
  notes: "IQL policy training with learned reward function"