# collect preferences using dtw pref augmentations
python3 collect_sequential_pref.py \
    data.data_path="/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt" \
    data.segment_length=64

# train reward model on preferences + save reward model analysis
python3 train_reward_model.py \
    data.data_path="/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt" \
    data.preferences_data_path="/scr/matthewh6/robot_pref/results/sequential_preferences/n50_k5_seed42_dtw5000/preference_dataset.pkl" \
    data.segment_length=64

# train iql using learned reward model
python train_policy.py --config-name=iql_robomimic \
    data.data_path="/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt" \
    data.reward_model_path="/scr/matthewh6/robot_pref/results/reward_model/mg_image_dense_n50_k5_model_seg64_hidden256_256_epochs100_pairs500/model.pt" \
    data.env_name="lift" \
    iql.weight_temp=1.0


# active offline learning [random, entropy, disagreement]
python train_reward_model_active.py \
    data.data_path="/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt" \
    active_learning.uncertainty_method=random \
    data.segment_length=64

# active offline learning with augmentations
python train_reward_model_active.py \
    data.data_path="/scr2/shared/pref/datasets/robomimic/lift/mg_image_dense.pt" \
    active_learning.uncertainty_method=random \
    data.segment_length=64 \
    dtw_augmentation.enabled=true


# bc
python train_policy.py --config-name=bc_robomimic \
    data.data_path=/scr2/shared/pref/datasets/robomimic/can/mg_image_dense.pt \
    data.env_name=can