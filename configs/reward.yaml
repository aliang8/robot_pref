device: cuda
env: robomimic_lift
seed: 0
checkpoints_path: logs/
load_model: null

# preference
feedback_num: 500 # Number of human preferences from the source dataset to use for augmentation
test_feedback_num: 100 # Number of human preferences from the target dataset to use for testing

data_quality: 5.0
segment_size: 32
normalize: false
threshold: 0.5
data_aug: none
q_budget: 1
feedback_type: RLT
model_type: BT
noise: 0.0
human: false
# MLP
epochs: 3000
batch_size: 256
activation: tanh
lr: 1e-3
hidden_sizes: 128
ensemble_num: 3
ensemble_method: mean
use_distributional_model: false # Use distributional reward model

# Reward model
use_gt_prefs: false # GT
eef_rm: false # 3D EEF RM
use_dtw_augmentations: false # DTW augs
dtw_k_augment: 1

# Data
data_path: "/scr/shared/datasets/robot_pref/lift_panda/lift_panda.hdf5"
target_data_path: "/scr/shared/datasets/robot_pref/lift_sawyer/lift_sawyer.hdf5"

# wandb
use_wandb: false
wandb:
  project: "Reward Learning"
  entity: "clvr"
  name: "RM-${env}-${seed}"